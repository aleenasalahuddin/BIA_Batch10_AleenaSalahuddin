{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Artificial Neural Network Model for predicting iris**\n"
      ],
      "metadata": {
        "id": "-O0eYXGH76X3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzZqC_ct6Xmd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Loading Iris**"
      ],
      "metadata": {
        "id": "sI67-qoH67KW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()"
      ],
      "metadata": {
        "id": "MIjysStH65Sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miKEb7o57Dru",
        "outputId": "90c4e0c0-0503-4f50-d3fb-29e102179fcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Converting data into x, y**"
      ],
      "metadata": {
        "id": "TkY_WTF77GUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = iris['data']\n",
        "y = iris['target']"
      ],
      "metadata": {
        "id": "FFGj7Cs87Lp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Standard Scaling of Features**\n"
      ],
      "metadata": {
        "id": "U4hloo3e7PWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()"
      ],
      "metadata": {
        "id": "RpzXY2xZ7T6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_scaled = scaler.fit_transform(X) # use to trnasform them into standar scaling"
      ],
      "metadata": {
        "id": "xQgrkf3T7WT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Conversion of data into tensors**"
      ],
      "metadata": {
        "id": "iyjTyo_K7atb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_tensor = torch.tensor(X_scaled, dtype=torch.float32) # converting them into tensor format\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)"
      ],
      "metadata": {
        "id": "AysYrEuj7ZG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Data Partitioning**"
      ],
      "metadata": {
        "id": "fC5e0oB87h24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2)"
      ],
      "metadata": {
        "id": "WszqpVQM7f7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = TensorDataset(X_train, y_train) # it is a requirement of the syntex for the model\n",
        "test_ds = TensorDataset(X_test, y_test)"
      ],
      "metadata": {
        "id": "eWJKNwX17m6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=16) # batch size should not be very big nor very small i.e. 8, 16"
      ],
      "metadata": {
        "id": "0lPs4ZaF7oxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AnnModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AnnModel, self).__init__()\n",
        "        # Define layers\n",
        "        self.fc1 = nn.Linear(4, 16)\n",
        "        self.Relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(16, 3)\n",
        "\n",
        "    # defining the flow of the data\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.Relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "Am7822cB7sWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AnnModel()"
      ],
      "metadata": {
        "id": "6e90LXO37uyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "citerion = nn.CrossEntropyLoss() # calculates the difference\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01) # updates the weights and biases at given rate"
      ],
      "metadata": {
        "id": "gh-lKJtq7xlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50 # one complete cycle through the entire training dataset during training"
      ],
      "metadata": {
        "id": "Bp1PbN-88AbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        optimizer.zero_grad() # optimizer changes weight and bias at given rate\n",
        "        outputs = model(batch_x) # this line applies model on batch_x (features) and store output in variable 'output'\n",
        "        loss = citerion(outputs, batch_y) # Forward Propogation\n",
        "        loss.backward() # Backward Propogation\n",
        "        optimizer.step() # changes the weights and bias according to learning\n",
        "        # by this point the modle is completed and trained the next lines are for personal evaluation\n",
        "        total_loss +=loss.item()\n",
        "    print(f\"Epoch: {epoch}, Loss: {total_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39Pu2t4x8Ez2",
        "outputId": "aa483566-7a57-426f-8d53-e774a13a4a67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 5.91192501783371\n",
            "Epoch: 1, Loss: 4.064800649881363\n",
            "Epoch: 2, Loss: 3.1265911757946014\n",
            "Epoch: 3, Loss: 2.504741534590721\n",
            "Epoch: 4, Loss: 2.2248829901218414\n",
            "Epoch: 5, Loss: 1.9495065212249756\n",
            "Epoch: 6, Loss: 1.751310020685196\n",
            "Epoch: 7, Loss: 1.6177548095583916\n",
            "Epoch: 8, Loss: 1.3733552396297455\n",
            "Epoch: 9, Loss: 1.242641806602478\n",
            "Epoch: 10, Loss: 1.0873157531023026\n",
            "Epoch: 11, Loss: 0.9614417664706707\n",
            "Epoch: 12, Loss: 0.9030695594847202\n",
            "Epoch: 13, Loss: 0.7670299671590328\n",
            "Epoch: 14, Loss: 0.6917162481695414\n",
            "Epoch: 15, Loss: 0.7281034253537655\n",
            "Epoch: 16, Loss: 0.6139706559479237\n",
            "Epoch: 17, Loss: 0.5991733632981777\n",
            "Epoch: 18, Loss: 0.5647460594773293\n",
            "Epoch: 19, Loss: 0.5138627737760544\n",
            "Epoch: 20, Loss: 0.5344760231673717\n",
            "Epoch: 21, Loss: 0.5306311175227165\n",
            "Epoch: 22, Loss: 0.45052226539701223\n",
            "Epoch: 23, Loss: 0.4552547112107277\n",
            "Epoch: 24, Loss: 0.545802092179656\n",
            "Epoch: 25, Loss: 0.46812252467498183\n",
            "Epoch: 26, Loss: 0.42932199500501156\n",
            "Epoch: 27, Loss: 0.4111561430618167\n",
            "Epoch: 28, Loss: 0.3940949810203165\n",
            "Epoch: 29, Loss: 0.42474975856021047\n",
            "Epoch: 30, Loss: 0.4011291041970253\n",
            "Epoch: 31, Loss: 0.4403914143331349\n",
            "Epoch: 32, Loss: 0.3632497328799218\n",
            "Epoch: 33, Loss: 0.37783824279904366\n",
            "Epoch: 34, Loss: 0.3642388783628121\n",
            "Epoch: 35, Loss: 0.3657860094681382\n",
            "Epoch: 36, Loss: 0.3996479269117117\n",
            "Epoch: 37, Loss: 0.36234648479148746\n",
            "Epoch: 38, Loss: 0.38037658063694835\n",
            "Epoch: 39, Loss: 0.3562671598047018\n",
            "Epoch: 40, Loss: 0.37667058734223247\n",
            "Epoch: 41, Loss: 0.371137329377234\n",
            "Epoch: 42, Loss: 0.42127432581037283\n",
            "Epoch: 43, Loss: 0.41102763498201966\n",
            "Epoch: 44, Loss: 0.36123324232175946\n",
            "Epoch: 45, Loss: 0.3622895037988201\n",
            "Epoch: 46, Loss: 0.3255036415939685\n",
            "Epoch: 47, Loss: 0.34125886857509613\n",
            "Epoch: 48, Loss: 0.5350708863697946\n",
            "Epoch: 49, Loss: 0.34704983001574874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad(): # this lines tells: don't change anything the model is already trained\n",
        "    lst = []\n",
        "    for batch_x, batch_y in test_loader:\n",
        "        outputs = model(batch_x) # the maximum number of output is the most likely prediction\n",
        "        _, prediction = torch.max(outputs, 1) # this function gives the position of maximum output i.e. 0, 1, 2\n",
        "        print(\"Prediction:    \", prediction)\n",
        "        print(\"Actual Results:\", batch_y) # now we are comparing the model predictions with actual results\n",
        "        lst.append(prediction) # a list containing all batches of predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mD-8ZdLH8HUf",
        "outputId": "b08a9211-97b5-4a89-f289-0eb70dcbf259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction:     tensor([0, 2, 2, 2, 0, 1, 1, 2, 0, 2, 2, 0, 2, 2, 1, 2])\n",
            "Actual Results: tensor([0, 2, 2, 2, 0, 1, 1, 2, 0, 1, 2, 0, 2, 2, 2, 2])\n",
            "Prediction:     tensor([1, 1, 2, 1, 0, 2, 0, 1, 2, 2, 1, 1, 0, 0])\n",
            "Actual Results: tensor([1, 1, 2, 1, 0, 2, 0, 1, 2, 2, 1, 1, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lst"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx_EQEGv8MW3",
        "outputId": "6be109c7-ceed-4e8f-bd7a-894eaf10e679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([0, 2, 2, 2, 0, 1, 1, 2, 0, 2, 2, 0, 2, 2, 1, 2]),\n",
              " tensor([1, 1, 2, 1, 0, 2, 0, 1, 2, 2, 1, 1, 0, 0])]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we are changing the predictions with actual names at given positions by manually looking at them\n",
        "for i in lst[0]:\n",
        "    if i.item() == 0:\n",
        "        print(\"Setosa\")\n",
        "    elif i.item() == 1:\n",
        "        print(\"Versicolor\")\n",
        "    else:\n",
        "        print(\"Virginica\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHEgEsQ18O_j",
        "outputId": "552d6cec-25d1-420c-cea1-ae15998a27e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setosa\n",
            "Virginica\n",
            "Virginica\n",
            "Virginica\n",
            "Setosa\n",
            "Versicolor\n",
            "Versicolor\n",
            "Virginica\n",
            "Setosa\n",
            "Virginica\n",
            "Virginica\n",
            "Setosa\n",
            "Virginica\n",
            "Virginica\n",
            "Versicolor\n",
            "Virginica\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris['target_names'] # this line shows the names of outputs and their positions that we used in above code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cx1WjGps8SuA",
        "outputId": "02957da6-312c-473f-c5bc-f767275524c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b46d342f"
      },
      "source": [
        "# **Analysis & Conclusion**\n",
        "Delving into the realm of artificial intelligence, this analysis focuses on an Artificial Neural Network (ANN) meticulously crafted for the classification of Iris species.\n",
        "\n",
        "The process commenced with the acquisition of the Iris dataset, a crucial step before segregating the features (X) and the target variable (y). A vital preprocessing phase involved standardizing the features using `StandardScaler`, a technique employed to optimize the training process.\n",
        "\n",
        "The standardized data was then transformed into PyTorch tensors and strategically divided into training and testing sets. The use of `TensorDataset` and `DataLoader` streamlined the process of handling data in batches during training.\n",
        "\n",
        "An ANN model, characterized by a single hidden layer incorporating `nn.Linear` and `nn.ReLU`, and a three-node output layer, was subsequently defined. The training employed `CrossEntropyLoss` as the loss function and the Adam optimizer with a learning rate of 0.01.\n",
        "\n",
        "Training spanned 50 epochs, during which a consistent decrease in loss was observed, signifying the model's successful learning progression.\n",
        "\n",
        "The model's efficacy was ultimately assessed on the test set. Predictions were generated and compared against the actual values. While a precise quantitative accuracy was not calculated, the comparison offered valuable insights into the model's predictive prowess, with numerical predictions seamlessly mapped to their corresponding species names via `target_names`."
      ]
    }
  ]
}